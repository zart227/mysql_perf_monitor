from jinja2 import Template
from datetime import datetime
import os
import pandas as pd
import io
import re
import logging
from report.ai_prompt_utils import build_ai_prompt
from core.ai_advisor import send_to_ai_advisor
import collections
from config.config import ENABLE_AI
import csv

logger = logging.getLogger(__name__)

REPORT_TEMPLATE = """
# –û—Ç—á—ë—Ç –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ MySQL

**–î–∞—Ç–∞:** {{ date }}

## –ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

{% if issues %}
### –ü—Ä–æ–±–ª–µ–º—ã:
{% for issue in issues %}
- {{ issue }}
{% endfor %}
{% else %}
–ö—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.
{% endif %}

{% if recommendations %}
### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
{% for rec in recommendations %}
- {{ rec }}
{% endfor %}
{% endif %}

---

## –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

{% for key, output in metrics.items() %}
{% if key != 'cpu_spikes' %}
### {{ key }}
{% if key in ['global_status', 'global_variables', 'qcache_status', 'processlist'] and '---' in output %}
{{ output }}
{% else %}
```
{{ output }}
```
{% endif %}
{% endif %}
{% endfor %}

{% if metrics.cpu_spikes %}
## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∏–∫–∞—Ö CPU

{% for spike in metrics.cpu_spikes %}
### –ü–∏–∫ –≤ {{ spike.timestamp }} (CPU: {{ spike.cpu_usage }}%)

**–ü—Ä–æ—Ü–µ—Å—Å-–≤–∏–Ω–æ–≤–Ω–∏–∫:**
```
{{ spike.triggering_process_line }}
```

**–°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–æ–º–µ–Ω—Ç –ø–∏–∫–∞ (`SHOW FULL PROCESSLIST`):**
{{ spike.processlist_output }}
---
{% endfor %}
{% endif %}
"""

BASELINE_TEMPLATE = """
# –ë–∞–∑–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ MySQL
**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** {{ date }}
---
## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ CPU
{{ metrics.cpuinfo }}
---
## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏
{{ metrics.memory }}
---
## –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ MySQL
{{ metrics.global_variables }}
"""

EVENT_HEADER_TEMPLATE = """
# –ñ—É—Ä–Ω–∞–ª —Å–æ–±—ã—Ç–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞ {{ date }}
"""

CPU_EVENT_TEMPLATE = """
---
### üìà –ü–∏–∫ CPU –≤ {{ time }}
- **PID –ø—Ä–æ—Ü–µ—Å—Å–∞:** `{{ pid }}`
- **–ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞:** `{{ cpu_percent }}%`

**–¢–æ–ø-5 –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –º–æ–º–µ–Ω—Ç –ø–∏–∫–∞:**
{{ process_list }}
"""

MEMORY_EVENT_TEMPLATE = """
---
### üìâ –í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ {{ time }}
- **–ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:** `{{ memory_percent }}%`
"""

def parse_innodb_status(status_string):
    """
    –ü–∞—Ä—Å–∏—Ç –≤—ã–≤–æ–¥ SHOW ENGINE INNODB STATUS, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –¥–≤—É—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö:
    1. –¢–∞–±–ª–∏—á–Ω—ã–π (—Å \t –∏ \n)
    2. –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π (—Å \G)
    """
    if "***************************" in status_string:
        # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (\G)
        match = re.search(r'Status:\n(.*?)$', status_string, re.DOTALL)
        if match:
            return match.group(1).strip()
    else:
        # –¢–∞–±–ª–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        parts = status_string.split('\t')
        if len(parts) > 2:
            return parts[2].replace('\\n', '\n').strip()
    
    # Fallback
    return status_string.replace('\\n', '\n').strip()

def to_markdown_table(data):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Å—Ç—Ä–æ–∫–∞ —Å —Ç–∞–±—É–ª—è—Ü–∏–µ–π –∏–ª–∏ markdown) –≤ markdown-—Ç–∞–±–ª–∏—Ü—É."""
    if not data or not isinstance(data, str):
        return data or ''
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–±—É–ª—è—Ü–∏–∏, –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ pandas
    if '\t' in data:
        try:
            df = pd.read_csv(io.StringIO(data), sep='\t', engine='python')
            return df.to_markdown(index=False)
        except Exception as e:
            return f"```\n(–æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–∞–±–ª–∏—Ü—ã: {e})\n{data}\n```"
    return data

def parse_and_format_free_output(free_output):
    """–ü–∞—Ä—Å–∏—Ç –≤—ã–≤–æ–¥ 'free -m' –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –µ–≥–æ –≤ –≤–∏–¥–µ markdown-—Ç–∞–±–ª–∏—Ü—ã –∏ —Ç–∞–±–ª–∏—Ü—ã buffers/cache."""
    if not free_output or not isinstance(free_output, str):
        return f"```\n{free_output or 'N/A'}\n```"
    try:
        lines = free_output.strip().splitlines()
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–∞–º—è—Ç–∏
        main_table = '\n'.join(lines[:3])
        main_table_md = to_markdown_table(main_table)
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è buffers/cache
        buffer_line = lines[2]
        buffer_parts = buffer_line.split()
        buffer_used = buffer_parts[2]
        buffer_free = buffer_parts[3]
        buffer_df = pd.DataFrame([
            {"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": "Used (-buffers/cache)", "–ó–Ω–∞—á–µ–Ω–∏–µ (MB)": buffer_used},
            {"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": "Free (+buffers/cache)", "–ó–Ω–∞—á–µ–Ω–∏–µ (MB)": buffer_free}
        ])
        table2 = buffer_df.to_markdown(index=False)
        return f"{main_table_md}\n\n**–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ `-/+ buffers/cache`:**\n{table2}"
    except Exception as e:
        return f"```\n(–æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ 'free -m': {e})\n{free_output}\n```"

def parse_and_format_cpuinfo(cpuinfo_output):
    """–ü–∞—Ä—Å–∏—Ç –≤—ã–≤–æ–¥ /proc/cpuinfo –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤ —Ç–∞–±–ª–∏—Ü—É "–ü–∞—Ä–∞–º–µ—Ç—Ä-–ó–Ω–∞—á–µ–Ω–∏–µ"."""
    if not cpuinfo_output or not isinstance(cpuinfo_output, str):
        return f"```\n{cpuinfo_output or 'N/A'}\n```"
    try:
        # --- –ë–ª–æ–∫ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –ø–æ –ø–µ—Ä–≤–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—É ---
        processor_blocks = cpuinfo_output.strip().split('\n\n')
        first_block = ""
        for block in processor_blocks:
            if block.strip():
                first_block = block
                break
        
        if not first_block.strip():
            processor_lines = cpuinfo_output.strip().split('\n')
            first_block_lines = []
            for line in processor_lines:
                if not line.strip() and first_block_lines:
                    break
                first_block_lines.append(line)
            first_block = "\n".join(first_block_lines)

        if not first_block.strip():
             return f"```\n(–Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –±–ª–æ–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –≤ cpuinfo)\n{cpuinfo_output}\n```"
        # --- –ö–æ–Ω–µ—Ü –±–ª–æ–∫–∞ ---

        params = []
        values = []
        for line in first_block.split('\n'):
            if ':' in line:
                parts = line.split(':', 1)
                key = parts[0].strip()
                value = parts[1].strip()
                params.append(key)
                values.append(value)

        if not params:
            return f"```\n(–Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å cpuinfo)\n{cpuinfo_output}\n```"

        df = pd.DataFrame({
            '–ü–∞—Ä–∞–º–µ—Ç—Ä': params,
            '–ó–Ω–∞—á–µ–Ω–∏–µ': values
        })
        
        return df.to_markdown(index=False)
    except Exception as e:
        return f"```\n(–æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ cpuinfo: {e})\n{cpuinfo_output}\n```"

def generate_report(metrics, issues, recommendations, output_path=None):
    processed_metrics = metrics.copy()
    
    table_alignments = {
        'global_status': ("left", "left"),
        'global_variables': ("left", "left"),
        'qcache_status': ("left", "right"),
        'processlist': ("right", "left", "left", "center", "left", "right", "center", "left")
    }
    
    table_keys = list(table_alignments.keys())
    
    for key, value in processed_metrics.items():
        if not value or not isinstance(value, str):
            continue

        if key in table_keys and '\t' in value:
            try:
                df = pd.read_csv(io.StringIO(value), sep='\\t', engine='python')
                colalign = table_alignments.get(key)
                if colalign and len(df.columns) != len(colalign):
                    colalign = None # Fallback to default if column count mismatches
                processed_metrics[key] = df.to_markdown(index=False, colalign=colalign)
            except Exception:
                processed_metrics[key] = f"```\n{value}\n```"

        elif key == 'innodb_status':
            processed_metrics[key] = parse_innodb_status(value)

    if 'cpu_spikes' in processed_metrics:
        for spike in processed_metrics.get('cpu_spikes', []):
            proc_list = spike.get('processlist_output')
            if proc_list and isinstance(proc_list, str) and '\t' in proc_list:
                try:
                    df = pd.read_csv(io.StringIO(proc_list), sep='\\t', engine='python')
                    colalign = table_alignments.get('processlist')
                    if colalign and len(df.columns) != len(colalign):
                        colalign = None # Fallback to default
                    spike['processlist_output'] = df.to_markdown(index=False, colalign=colalign)
                except Exception:
                    spike['processlist_output'] = f"```\n{proc_list}\n```"

    template = Template(REPORT_TEMPLATE)
    report = template.render(
        date=datetime.now().strftime('%Y-%m-%d %H:%M'),
        metrics=processed_metrics,
        issues=issues,
        recommendations=recommendations
    )
    if output_path:
        abs_path = os.path.join(os.getcwd(), output_path) if not os.path.isabs(output_path) else output_path
        os.makedirs(os.path.dirname(abs_path), exist_ok=True)
        with open(abs_path, 'w', encoding='utf-8') as f:
            f.write(report)
    return report 

def generate_baseline_report(metrics, output_path):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å cpuinfo, memory –∏ global_variables."""
    processed_metrics = {
        'cpuinfo': parse_and_format_cpuinfo(metrics.get('cpuinfo', 'N/A')),
        'memory': parse_and_format_free_output(metrics.get('memory', 'N/A')),
        'global_variables': to_markdown_table(metrics.get('global_variables'))
    }

    template = Template(BASELINE_TEMPLATE)
    report = template.render(
        date=datetime.now().strftime('%Y-%m-%d %H:%M'),
        metrics=processed_metrics
    )
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

def _ensure_header(report_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫, –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∏—Ö –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
    date_str = datetime.now().strftime('%Y-%m-%d')
    header = Template(EVENT_HEADER_TEMPLATE).render(date=date_str)
    
    if not os.path.exists(report_path):
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(header)
            f.write('\n')

def append_cpu_event_to_report(event_data, report_path):
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∏–∫–µ CPU –≤ –æ—Ç—á–µ—Ç –æ —Å–æ–±—ã—Ç–∏—è—Ö (markdown, –∫–∞–∫ —Ä–∞–Ω—å—à–µ) –∏ –≤ CSV (–ø–ª–æ—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç: –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å, info –±–µ–∑ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫).
    """
    import re
    try:
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        csv_path = os.path.join(os.path.dirname(report_path), 'events_cpu.csv')
        csv_exists = os.path.exists(csv_path)
        process_list = event_data.get('process_list', '')
        # --- –ü–∞—Ä—Å–∏–º process_list –¥–ª—è CSV ---
        queries = []
        if process_list and process_list.strip():
            lines = process_list.strip().splitlines()
            header_line = None
            for line in lines:
                if line.startswith('|') and 'INFO' in line.upper():
                    header_line = [h.strip().upper() for h in line.split('|')[1:-1]]
                    break
            
            if header_line:
                try:
                    # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å—ã –Ω—É–∂–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤
                    user_idx = header_line.index('USER')
                    host_idx = header_line.index('HOST')
                    time_idx = header_line.index('TIME')
                    info_idx = header_line.index('INFO')

                    for line in lines:
                        if line.startswith('|') and not line.startswith('+-') and 'USER' not in line.upper():
                            parts = [p.strip() for p in line.split('|')[1:-1]]
                            if len(parts) > max(user_idx, host_idx, time_idx, info_idx):
                                user = parts[user_idx]
                                host = parts[host_idx]
                                time_val = parts[time_idx]
                                info = parts[info_idx].replace('\n', ' ').replace('\r', ' ')
                                info = re.sub(r'\s+', ' ', info)
                                if info and info != 'NULL':
                                    queries.append({'user': user, 'host': host, 'time_query': time_val, 'info': info})
                except ValueError:
                    logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã (USER, HOST, TIME, INFO) –≤ –≤—ã–≤–æ–¥–µ processlist.")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ processlist: {e}", exc_info=True)


        # --- –ó–∞–ø–∏—Å—å –≤ CSV ---
        if queries:
            with open(csv_path, 'a', newline='', encoding='utf-8') as csvfile:
                fieldnames = ['date', 'time', 'pid', 'cpu', 'user', 'host', 'time_query', 'info']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_ALL)
                if not csv_exists:
                    writer.writeheader()
                for q in queries:
                    writer.writerow({
                        'date': datetime.now().strftime('%Y-%m-%d'),
                        'time': event_data['time'],
                        'pid': event_data['pid'],
                        'cpu': event_data['cpu'],
                        'user': q['user'],
                        'host': q['host'],
                        'time_query': q['time_query'],
                        'info': q['info'],
                    })
        # --- Markdown-–æ—Ç—á—ë—Ç (–∫–∞–∫ —Ä–∞–Ω—å—à–µ) ---
        if not os.path.exists(report_path):
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("# üìä –û—Ç—á–µ—Ç –æ —Å–æ–±—ã—Ç–∏—è—Ö –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ MySQL\n\n")
        time_str = event_data['time']
        cpu_usage = event_data['cpu']
        pid = event_data['pid']
        # –ï—Å–ª–∏ process_list ‚Äî —Ç–∞–±–ª–∏—Ü–∞, –≤—Å—Ç–∞–≤–ª—è–µ–º –µ—ë –∫–∞–∫ –µ—Å—Ç—å, –∏–Ω–∞—á–µ –ø–∏—à–µ–º '–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.'
        if process_list and process_list.strip().startswith('+'):
            processlist_md = f'''```
{process_list.strip()}
```'''
        elif queries:
            # fallback: –µ—Å–ª–∏ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞, –Ω–æ –µ—Å—Ç—å —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            processlist_md = '| user | host | time | info |\n|---|---|---|---|\n' + '\n'.join(
                f"| {q['user']} | {q['host']} | {q['time_query']} | {q['info'][:100]}... |" for q in queries
            )
        else:
            processlist_md = '–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.'
        event_entry = f"""
---
### üìà –ü–∏–∫ CPU –≤ {time_str}
- **PID –ø—Ä–æ—Ü–µ—Å—Å–∞:** `{pid}`
- **–ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞:** `{cpu_usage}%`

**–¢–æ–ø-5 –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –º–æ–º–µ–Ω—Ç –ø–∏–∫–∞:**
{processlist_md}

"""
        performance_analysis = event_data.get('performance_analysis')
        if performance_analysis:
            event_entry += f"""
**üìä –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤:**
- **–í—Å–µ–≥–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:** {performance_analysis['total_queries']}
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {performance_analysis['max_time']} —Å–µ–∫
- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {performance_analysis['avg_time']:.1f} —Å–µ–∫
- **–ú–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (>10 —Å–µ–∫):** {len(performance_analysis['slow_queries'])}
- **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (>30 —Å–µ–∫):** {len(performance_analysis['critical_queries'])}

"""
            if performance_analysis['critical_queries']:
                event_entry += "**üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (>30 —Å–µ–∫):**\n"
                for query in performance_analysis['critical_queries']:
                    info = str(query.get('INFO', 'N/A')).replace('\n', ' ').replace('\r', ' ')
                    info = re.sub(r'\s+', ' ', info)
                    event_entry += f"- **{query['TIME']} —Å–µ–∫:** {info[:100]}...\n"
                event_entry += "\n"
            elif performance_analysis['slow_queries']:
                event_entry += "**‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (>10 —Å–µ–∫):**\n"
                for query in performance_analysis['slow_queries']:
                    info = str(query.get('INFO', 'N/A')).replace('\n', ' ').replace('\r', ' ')
                    info = re.sub(r'\s+', ' ', info)
                    event_entry += f"- **{query['TIME']} —Å–µ–∫:** {info[:100]}...\n"
                event_entry += "\n"
        with open(report_path, 'a', encoding='utf-8') as f:
            f.write(event_entry)
        logger.info(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∏–∫–µ CPU –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –æ—Ç—á–µ—Ç: {report_path}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∏–∫–µ CPU –≤ –æ—Ç—á–µ—Ç: {e}", exc_info=True)

def append_memory_event_to_report(event_data, output_path):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –≤ –æ—Ç—á–µ—Ç —Å–æ–±—ã—Ç–∏–µ –æ –≤—ã—Å–æ–∫–æ–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–∏ –ø–∞–º—è—Ç–∏ –∏ –≤ CSV."""
    _ensure_header(output_path)
    # CSV-—Ñ–∞–π–ª –¥–ª—è –ø–∞–º—è—Ç–∏
    csv_path = os.path.join(os.path.dirname(output_path), 'events_memory.csv')
    csv_exists = os.path.exists(csv_path)
    with open(csv_path, 'a', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile, quoting=csv.QUOTE_ALL)
        if not csv_exists:
            writer.writerow(['date', 'time', 'memory_percent'])
        writer.writerow([
            datetime.now().strftime('%Y-%m-%d'),
            event_data['time'],
            event_data['memory_percent']
        ])
    template = Template(MEMORY_EVENT_TEMPLATE)
    report_content = template.render(
        time=event_data['time'],
        memory_percent=event_data['memory_percent']
    )
    with open(output_path, 'a', encoding='utf-8') as f:
        f.write(report_content)

def check_if_memory_event_exists(report_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–æ –ª–∏ —É–∂–µ —Å–µ–≥–æ–¥–Ω—è —Å–æ–±—ã—Ç–∏–µ –ø–æ –ø–∞–º—è—Ç–∏."""
    if not os.path.exists(report_path):
        return False
    with open(report_path, 'r', encoding='utf-8') as f:
        content = f.read()
    return '–í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏' in content 

def parse_and_aggregate_events(events_path):
    """
    –ü–∞—Ä—Å–∏—Ç events_report_YYYYMMDD.md –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç:
    - –∑–∞–≥—Ä—É–∑–∫—É CPU (–º–∞–∫—Å/–º–∏–Ω/—Å—Ä–µ–¥–Ω–µ–µ)
    - –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã (–≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –ø–æ INFO)
    - –º–µ–¥–ª–µ–Ω–Ω—ã–µ/–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã (–º–µ–¥–ª–µ–Ω–Ω—ã–µ >1 —Å–µ–∫)
    - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    """
    if not os.path.exists(events_path):
        logger.warning(f"–§–∞–π–ª —Å–æ–±—ã—Ç–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {events_path}")
        return {}
    
    with open(events_path, encoding='utf-8') as f:
        text = f.read()
    
    logger.info(f"–ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞ —Å–æ–±—ã—Ç–∏–π: {events_path}, —Ä–∞–∑–º–µ—Ä: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –ü–∞—Ä—Å–∏–º –ø–∏–∫–∏ CPU
    cpu_usages = []
    all_queries = []
    slow_queries = []
    critical_queries = []
    query_times = []
    query_groups = collections.defaultdict(list)
    
    # –ò—â–µ–º –≤—Å–µ –ø–∏–∫–∏ CPU –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º ### üìà –ü–∏–∫ CPU
    cpu_peaks = re.findall(r'### üìà –ü–∏–∫ CPU –≤ (\d{2}:\d{2}:\d{2})[\s\S]*?–ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞:\s*`([\d\.]+)%`', text)
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–∏–∫–æ–≤ CPU –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º: {len(cpu_peaks)}")
    
    for time_str, cpu_usage in cpu_peaks:
        cpu_usage = float(cpu_usage)
        cpu_usages.append(cpu_usage)
        logger.info(f"–ù–∞–π–¥–µ–Ω –ø–∏–∫ CPU –≤ {time_str}: {cpu_usage}%")
    
    # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–æ–≤
    table_matches = re.findall(r'\|\s*ID\s*\|.*?\n((?:\|.*?\n)+)', text, re.DOTALL)
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü –∑–∞–ø—Ä–æ—Å–æ–≤: {len(table_matches)}")
    
    for i, table in enumerate(table_matches):
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ç–∞–±–ª–∏—Ü—É {i+1}")
        # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
        for line in table.strip().split('\n'):
            if not line.strip().startswith('|'):
                continue
            parts = [p.strip() for p in line.strip('|').split('|')]
            if len(parts) < 7:
                continue
            try:
                q_id, user, host, db, command, time_val, state, info = parts[:8]
                time_val = int(time_val)
                query = {
                    'ID': q_id,
                    'USER': user,
                    'HOST': host,
                    'DB': db,
                    'COMMAND': command,
                    'TIME': time_val,
                    'STATE': state,
                    'INFO': info
                }
                all_queries.append(query)
                query_times.append(time_val)
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ INFO (–æ–±—Ä–µ–∑–∞–µ–º –¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏)
                group_key = info[:100]
                query_groups[group_key].append(query)
                if time_val > 30:
                    critical_queries.append(query)
                elif time_val > 1:
                    slow_queries.append(query)
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã: {e}")
                continue
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ –ø–∏–∫–æ–≤ CPU: {len(cpu_usages)}")
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(all_queries)}")
    logger.info(f"–ú–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(slow_queries)}")
    logger.info(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {len(critical_queries)}")
    
    # –ê–≥—Ä–µ–≥–∞—Ç—ã
    cpu_agg = {
        'max': max(cpu_usages) if cpu_usages else None,
        'min': min(cpu_usages) if cpu_usages else None,
        'avg': sum(cpu_usages)/len(cpu_usages) if cpu_usages else None,
        'count': len(cpu_usages)
    }
    query_time_agg = {
        'max': max(query_times) if query_times else None,
        'min': min(query_times) if query_times else None,
        'avg': sum(query_times)/len(query_times) if query_times else None,
        'count': len(query_times)
    }
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    grouped_queries = []
    for key, group in query_groups.items():
        grouped_queries.append({
            'INFO': key,
            'count': len(group),
            'avg_time': sum(q['TIME'] for q in group)/len(group),
            'max_time': max(q['TIME'] for q in group),
            'min_time': min(q['TIME'] for q in group)
        })
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
    grouped_queries = sorted(grouped_queries, key=lambda x: x['count'], reverse=True)
    return {
        'cpu_agg': cpu_agg,
        'query_time_agg': query_time_agg,
        'grouped_queries': grouped_queries,
        'slow_queries': slow_queries,
        'critical_queries': critical_queries
    }

def generate_daily_summary_report(baseline_path, events_path, output_path):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –¥–Ω–µ–≤–Ω–æ–π –æ—Ç—á—ë—Ç —Å AI-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –∏ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–≤–æ–¥–∫–æ–π.
    –¢–µ–ø–µ—Ä—å –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç events_cpu.csv (–ø–ª–æ—Å–∫–∏–π —Ñ–æ—Ä–º–∞—Ç) –¥–ª—è CPU –∏ –∑–∞–ø—Ä–æ—Å–æ–≤.
    """
    import pandas as pd
    today = datetime.now().strftime('%Y-%m-%d')
    date_str = today
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è AI
    prompt = build_ai_prompt(baseline_path, events_path)
    if ENABLE_AI:
        try:
            ai_recommendations = send_to_ai_advisor(prompt)
        except Exception as e:
            ai_recommendations = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ AI: {e}"
    else:
        ai_recommendations = 'AI –æ—Ç–∫–ª—é—á—ë–Ω –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏.'
    # --- –ù–æ–≤—ã–π –±–ª–æ–∫: —á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ events_cpu.csv ---
    cpu_csv = os.path.join(os.path.dirname(events_path), 'events_cpu.csv')
    mem_csv = os.path.join(os.path.dirname(events_path), 'events_memory.csv')
    cpu_summary = ''
    mem_summary = ''
    if os.path.exists(cpu_csv):
        df = pd.read_csv(cpu_csv)
        df = df[df['date'] == date_str]
        if not df.empty:
            cpu_summary = (
                f"**CPU:**\n"
                f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(df)}\n"
                f"  - –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ CPU: {df['cpu'].mean():.1f}%\n"
                f"  - –ú–∞–∫—Å–∏–º—É–º: {df['cpu'].max()}%\n"
                f"  - –ú–∏–Ω–∏–º—É–º: {df['cpu'].min()}%\n"
            )
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤
            df['time_query'] = pd.to_numeric(df['time_query'], errors='coerce').fillna(0)
            query_time_agg = (
                f"**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤:**\n"
                f"  - –°—Ä–µ–¥–Ω–µ–µ: {df['time_query'].mean():.1f} —Å–µ–∫\n"
                f"  - –ú–∞–∫—Å–∏–º—É–º: {df['time_query'].max()} —Å–µ–∫\n"
                f"  - –ú–∏–Ω–∏–º—É–º: {df['time_query'].min()} —Å–µ–∫\n"
            )
            # –¢–æ–ø-5 –¥–æ–ª–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            top_long = df.sort_values('time_query', ascending=False).head(5)
            top_long_str = '\n'.join([
                f"  - {row['user']}@{row['host']} ({row['time_query']} —Å–µ–∫): {str(row['info'])[:100]}..." for _, row in top_long.iterrows()
            ])
            # –¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–æ info) —Å —Å—Ä–µ–¥–Ω–µ–π –∑–∞–≥—Ä—É–∑–∫–æ–π CPU
            top_freq_df = df.groupby('info').agg(
                count=('info', 'size'),
                avg_cpu=('cpu', 'mean')
            ).sort_values('count', ascending=False).head(5)

            top_freq_str = '\n'.join([
                f"  - {info[:100]}... (–≤—Å–µ–≥–æ: {row['count']}, —Å—Ä. CPU: {row['avg_cpu']:.1f}%)" 
                for info, row in top_freq_df.iterrows()
            ])

            cpu_summary += f"\n{query_time_agg}\n**–¢–æ–ø-5 –¥–æ–ª–≥–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:**\n{top_long_str}\n\n**–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:**\n{top_freq_str}\n"
    if os.path.exists(mem_csv):
        dfm = pd.read_csv(mem_csv)
        dfm = dfm[dfm['date'] == date_str]
        if not dfm.empty:
            mem_summary = (
                f"**–ü–∞–º—è—Ç—å:**\n"
                f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–±—ã—Ç–∏–π: {len(dfm)}\n"
                f"  - –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {dfm['memory_percent'].mean():.1f}%\n"
                f"  - –ú–∞–∫—Å–∏–º—É–º: {dfm['memory_percent'].max()}%\n"
                f"  - –ú–∏–Ω–∏–º—É–º: {dfm['memory_percent'].min()}%\n"
            )
    summary_str = cpu_summary + ('\n' if cpu_summary and mem_summary else '') + mem_summary
    # –§–æ—Ä–º–∏—Ä—É–µ–º baseline-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ, –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ baseline)
    key_params = prompt.split('–í–æ—Ç —Å–≤–æ–¥–∫–∞ —Å–æ–±—ã—Ç–∏–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è:')[0].replace('–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä MySQL. –í–æ—Ç –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ä–≤–µ—Ä–∞:', '').strip()
    # –ò—Ç–æ–≥–æ–≤—ã–π markdown-–æ—Ç—á—ë—Ç
    report = f"""
# –°–≤–æ–¥–Ω—ã–π –æ—Ç—á—ë—Ç –∑–∞ {date_str}

## –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã MySQL
{key_params}

## –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞ –∑–∞ –¥–µ–Ω—å
{summary_str}

## AI-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é)
{ai_recommendations}
"""
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    return report 