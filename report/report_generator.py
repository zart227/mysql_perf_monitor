from jinja2 import Template
from datetime import datetime
import os
import pandas as pd
import io
import re
import logging
from report.ai_prompt_utils import build_ai_prompt
from core.ai_advisor import send_to_ai_advisor
import collections
from config.config import ENABLE_AI

logger = logging.getLogger(__name__)

REPORT_TEMPLATE = """
# –û—Ç—á—ë—Ç –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ MySQL

**–î–∞—Ç–∞:** {{ date }}

## –ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

{% if issues %}
### –ü—Ä–æ–±–ª–µ–º—ã:
{% for issue in issues %}
- {{ issue }}
{% endfor %}
{% else %}
–ö—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.
{% endif %}

{% if recommendations %}
### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
{% for rec in recommendations %}
- {{ rec }}
{% endfor %}
{% endif %}

---

## –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏

{% for key, output in metrics.items() %}
{% if key != 'cpu_spikes' %}
### {{ key }}
{% if key in ['global_status', 'global_variables', 'qcache_status', 'processlist'] and '---' in output %}
{{ output }}
{% else %}
```
{{ output }}
```
{% endif %}
{% endif %}
{% endfor %}

{% if metrics.cpu_spikes %}
## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∏–∫–∞—Ö CPU

{% for spike in metrics.cpu_spikes %}
### –ü–∏–∫ –≤ {{ spike.timestamp }} (CPU: {{ spike.cpu_usage }}%)

**–ü—Ä–æ—Ü–µ—Å—Å-–≤–∏–Ω–æ–≤–Ω–∏–∫:**
```
{{ spike.triggering_process_line }}
```

**–°–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –º–æ–º–µ–Ω—Ç –ø–∏–∫–∞ (`SHOW FULL PROCESSLIST`):**
{{ spike.processlist_output }}
---
{% endfor %}
{% endif %}
"""

BASELINE_TEMPLATE = """
# –ë–∞–∑–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ MySQL
**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** {{ date }}
---
## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ CPU
{{ metrics.cpuinfo }}
---
## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏
{{ metrics.memory }}
---
## –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ MySQL
{{ metrics.global_variables }}
"""

EVENT_HEADER_TEMPLATE = """
# –ñ—É—Ä–Ω–∞–ª —Å–æ–±—ã—Ç–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞ {{ date }}
"""

CPU_EVENT_TEMPLATE = """
---
### üìà –ü–∏–∫ CPU –≤ {{ time }}
- **PID –ø—Ä–æ—Ü–µ—Å—Å–∞:** `{{ pid }}`
- **–ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞:** `{{ cpu_percent }}%`

**–¢–æ–ø-5 –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –º–æ–º–µ–Ω—Ç –ø–∏–∫–∞:**
{{ process_list }}
"""

MEMORY_EVENT_TEMPLATE = """
---
### üìâ –í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ {{ time }}
- **–ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:** `{{ memory_percent }}%`
"""

def parse_innodb_status(status_string):
    """
    –ü–∞—Ä—Å–∏—Ç –≤—ã–≤–æ–¥ SHOW ENGINE INNODB STATUS, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –¥–≤—É—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö:
    1. –¢–∞–±–ª–∏—á–Ω—ã–π (—Å \t –∏ \n)
    2. –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π (—Å \G)
    """
    if "***************************" in status_string:
        # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç (\G)
        match = re.search(r'Status:\n(.*?)$', status_string, re.DOTALL)
        if match:
            return match.group(1).strip()
    else:
        # –¢–∞–±–ª–∏—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        parts = status_string.split('\t')
        if len(parts) > 2:
            return parts[2].replace('\\n', '\n').strip()
    
    # Fallback
    return status_string.replace('\\n', '\n').strip()

def to_markdown_table(data):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–∞–±–ª–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Å—Ç—Ä–æ–∫–∞ —Å —Ç–∞–±—É–ª—è—Ü–∏–µ–π –∏–ª–∏ markdown) –≤ markdown-—Ç–∞–±–ª–∏—Ü—É."""
    if not data or not isinstance(data, str):
        return data or ''
    # –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–±—É–ª—è—Ü–∏–∏, –ø—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ pandas
    if '\t' in data:
        try:
            df = pd.read_csv(io.StringIO(data), sep='\t', engine='python')
            return df.to_markdown(index=False)
        except Exception as e:
            return f"```\n(–æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–∞–±–ª–∏—Ü—ã: {e})\n{data}\n```"
    return data

def parse_and_format_free_output(free_output):
    """–ü–∞—Ä—Å–∏—Ç –≤—ã–≤–æ–¥ 'free -m' –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –µ–≥–æ –≤ –≤–∏–¥–µ markdown-—Ç–∞–±–ª–∏—Ü—ã –∏ —Ç–∞–±–ª–∏—Ü—ã buffers/cache."""
    if not free_output or not isinstance(free_output, str):
        return f"```\n{free_output or 'N/A'}\n```"
    try:
        lines = free_output.strip().splitlines()
        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–∞–º—è—Ç–∏
        main_table = '\n'.join(lines[:3])
        main_table_md = to_markdown_table(main_table)
        # –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è buffers/cache
        buffer_line = lines[2]
        buffer_parts = buffer_line.split()
        buffer_used = buffer_parts[2]
        buffer_free = buffer_parts[3]
        buffer_df = pd.DataFrame([
            {"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": "Used (-buffers/cache)", "–ó–Ω–∞—á–µ–Ω–∏–µ (MB)": buffer_used},
            {"–ü–æ–∫–∞–∑–∞—Ç–µ–ª—å": "Free (+buffers/cache)", "–ó–Ω–∞—á–µ–Ω–∏–µ (MB)": buffer_free}
        ])
        table2 = buffer_df.to_markdown(index=False)
        return f"{main_table_md}\n\n**–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∞ `-/+ buffers/cache`:**\n{table2}"
    except Exception as e:
        return f"```\n(–æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ 'free -m': {e})\n{free_output}\n```"

def parse_and_format_cpuinfo(cpuinfo_output):
    """–ü–∞—Ä—Å–∏—Ç –≤—ã–≤–æ–¥ /proc/cpuinfo –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –≤ —Ç–∞–±–ª–∏—Ü—É "–ü–∞—Ä–∞–º–µ—Ç—Ä-–ó–Ω–∞—á–µ–Ω–∏–µ"."""
    if not cpuinfo_output or not isinstance(cpuinfo_output, str):
        return f"```\n{cpuinfo_output or 'N/A'}\n```"
    try:
        # --- –ë–ª–æ–∫ –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –ø–æ –ø–µ—Ä–≤–æ–º—É –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—É ---
        processor_blocks = cpuinfo_output.strip().split('\n\n')
        first_block = ""
        for block in processor_blocks:
            if block.strip():
                first_block = block
                break
        
        if not first_block.strip():
            processor_lines = cpuinfo_output.strip().split('\n')
            first_block_lines = []
            for line in processor_lines:
                if not line.strip() and first_block_lines:
                    break
                first_block_lines.append(line)
            first_block = "\n".join(first_block_lines)

        if not first_block.strip():
             return f"```\n(–Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –±–ª–æ–∫ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –≤ cpuinfo)\n{cpuinfo_output}\n```"
        # --- –ö–æ–Ω–µ—Ü –±–ª–æ–∫–∞ ---

        params = []
        values = []
        for line in first_block.split('\n'):
            if ':' in line:
                parts = line.split(':', 1)
                key = parts[0].strip()
                value = parts[1].strip()
                params.append(key)
                values.append(value)

        if not params:
            return f"```\n(–Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å cpuinfo)\n{cpuinfo_output}\n```"

        df = pd.DataFrame({
            '–ü–∞—Ä–∞–º–µ—Ç—Ä': params,
            '–ó–Ω–∞—á–µ–Ω–∏–µ': values
        })
        
        return df.to_markdown(index=False)
    except Exception as e:
        return f"```\n(–æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ cpuinfo: {e})\n{cpuinfo_output}\n```"

def generate_report(metrics, issues, recommendations, output_path=None):
    processed_metrics = metrics.copy()
    
    table_alignments = {
        'global_status': ("left", "left"),
        'global_variables': ("left", "left"),
        'qcache_status': ("left", "right"),
        'processlist': ("right", "left", "left", "center", "left", "right", "center", "left")
    }
    
    table_keys = list(table_alignments.keys())
    
    for key, value in processed_metrics.items():
        if not value or not isinstance(value, str):
            continue

        if key in table_keys and '\t' in value:
            try:
                df = pd.read_csv(io.StringIO(value), sep='\\t', engine='python')
                colalign = table_alignments.get(key)
                if colalign and len(df.columns) != len(colalign):
                    colalign = None # Fallback to default if column count mismatches
                processed_metrics[key] = df.to_markdown(index=False, colalign=colalign)
            except Exception:
                processed_metrics[key] = f"```\n{value}\n```"

        elif key == 'innodb_status':
            processed_metrics[key] = parse_innodb_status(value)

    if 'cpu_spikes' in processed_metrics:
        for spike in processed_metrics.get('cpu_spikes', []):
            proc_list = spike.get('processlist_output')
            if proc_list and isinstance(proc_list, str) and '\t' in proc_list:
                try:
                    df = pd.read_csv(io.StringIO(proc_list), sep='\\t', engine='python')
                    colalign = table_alignments.get('processlist')
                    if colalign and len(df.columns) != len(colalign):
                        colalign = None # Fallback to default
                    spike['processlist_output'] = df.to_markdown(index=False, colalign=colalign)
                except Exception:
                    spike['processlist_output'] = f"```\n{proc_list}\n```"

    template = Template(REPORT_TEMPLATE)
    report = template.render(
        date=datetime.now().strftime('%Y-%m-%d %H:%M'),
        metrics=processed_metrics,
        issues=issues,
        recommendations=recommendations
    )
    if output_path:
        abs_path = os.path.join(os.getcwd(), output_path) if not os.path.isabs(output_path) else output_path
        os.makedirs(os.path.dirname(abs_path), exist_ok=True)
        with open(abs_path, 'w', encoding='utf-8') as f:
            f.write(report)
    return report 

def generate_baseline_report(metrics, output_path):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–π –æ—Ç—á–µ—Ç —Å cpuinfo, memory –∏ global_variables."""
    processed_metrics = {
        'cpuinfo': parse_and_format_cpuinfo(metrics.get('cpuinfo', 'N/A')),
        'memory': parse_and_format_free_output(metrics.get('memory', 'N/A')),
        'global_variables': to_markdown_table(metrics.get('global_variables'))
    }

    template = Template(BASELINE_TEMPLATE)
    report = template.render(
        date=datetime.now().strftime('%Y-%m-%d %H:%M'),
        metrics=processed_metrics
    )
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)

def _ensure_header(report_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫, –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –∏—Ö –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏."""
    date_str = datetime.now().strftime('%Y-%m-%d')
    header = Template(EVENT_HEADER_TEMPLATE).render(date=date_str)
    
    if not os.path.exists(report_path):
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(header)
            f.write('\n')

def append_cpu_event_to_report(event_data, report_path):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∏–∫–µ CPU –≤ –æ—Ç—á–µ—Ç –æ —Å–æ–±—ã—Ç–∏—è—Ö."""
    try:
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs(os.path.dirname(report_path), exist_ok=True)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª
        if not os.path.exists(report_path):
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write("# üìä –û—Ç—á–µ—Ç –æ —Å–æ–±—ã—Ç–∏—è—Ö –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ MySQL\n\n")
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
        time_str = event_data['time']
        cpu_usage = event_data['cpu']
        pid = event_data['pid']
        process_list = event_data.get('process_list', '')
        performance_analysis = event_data.get('performance_analysis')
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –æ —Å–æ–±—ã—Ç–∏–∏
        event_entry = f"""
---
### üìà –ü–∏–∫ CPU –≤ {time_str}
- **PID –ø—Ä–æ—Ü–µ—Å—Å–∞:** `{pid}`
- **–ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞:** `{cpu_usage}%`

**–¢–æ–ø-5 –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –º–æ–º–µ–Ω—Ç –ø–∏–∫–∞:**
{to_markdown_table(process_list)}

"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if performance_analysis:
            event_entry += f"""
**üìä –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤:**
- **–í—Å–µ–≥–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:** {performance_analysis['total_queries']}
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {performance_analysis['max_time']} —Å–µ–∫
- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:** {performance_analysis['avg_time']:.1f} —Å–µ–∫
- **–ú–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (>10 —Å–µ–∫):** {len(performance_analysis['slow_queries'])}
- **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (>30 —Å–µ–∫):** {len(performance_analysis['critical_queries'])}

"""
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            if performance_analysis['critical_queries']:
                event_entry += "**üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (>30 —Å–µ–∫):**\n"
                for query in performance_analysis['critical_queries']:
                    event_entry += f"- **{query['TIME']} —Å–µ–∫:** {query.get('INFO', 'N/A')[:100]}...\n"
                event_entry += "\n"
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            elif performance_analysis['slow_queries']:
                event_entry += "**‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (>10 —Å–µ–∫):**\n"
                for query in performance_analysis['slow_queries']:
                    event_entry += f"- **{query['TIME']} —Å–µ–∫:** {query.get('INFO', 'N/A')[:100]}...\n"
                event_entry += "\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –≤ —Ñ–∞–π–ª
        with open(report_path, 'a', encoding='utf-8') as f:
            f.write(event_entry)
            
        logger.info(f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∏–∫–µ CPU –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –æ—Ç—á–µ—Ç: {report_path}")
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∏–∫–µ CPU –≤ –æ—Ç—á–µ—Ç: {e}", exc_info=True)

def append_memory_event_to_report(event_data, output_path):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –≤ –æ—Ç—á–µ—Ç —Å–æ–±—ã—Ç–∏–µ –æ –≤—ã—Å–æ–∫–æ–º –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–∏ –ø–∞–º—è—Ç–∏."""
    _ensure_header(output_path)
    
    template = Template(MEMORY_EVENT_TEMPLATE)
    report_content = template.render(
        time=event_data['time'],
        memory_percent=event_data['memory_percent']
    )
    
    with open(output_path, 'a', encoding='utf-8') as f:
        f.write(report_content)

def check_if_memory_event_exists(report_path):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–æ –ª–∏ —É–∂–µ —Å–µ–≥–æ–¥–Ω—è —Å–æ–±—ã—Ç–∏–µ –ø–æ –ø–∞–º—è—Ç–∏."""
    if not os.path.exists(report_path):
        return False
    with open(report_path, 'r', encoding='utf-8') as f:
        content = f.read()
    return '–í—ã—Å–æ–∫–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏' in content 

def parse_and_aggregate_events(events_path):
    """
    –ü–∞—Ä—Å–∏—Ç events_report_YYYYMMDD.md –∏ –∞–≥—Ä–µ–≥–∏—Ä—É–µ—Ç:
    - –∑–∞–≥—Ä—É–∑–∫—É CPU (–º–∞–∫—Å/–º–∏–Ω/—Å—Ä–µ–¥–Ω–µ–µ)
    - –≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã (–≥—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –ø–æ INFO)
    - –º–µ–¥–ª–µ–Ω–Ω—ã–µ/–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã (–º–µ–¥–ª–µ–Ω–Ω—ã–µ >1 —Å–µ–∫)
    - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    """
    if not os.path.exists(events_path):
        return {}
    with open(events_path, encoding='utf-8') as f:
        text = f.read()
    # –ü–∞—Ä—Å–∏–º –ø–∏–∫–∏ CPU
    cpu_usages = []
    all_queries = []
    slow_queries = []
    critical_queries = []
    query_times = []
    query_groups = collections.defaultdict(list)
    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –±–ª–æ–∫–∏ "–ü–∏–∫ CPU ..."
    cpu_blocks = re.split(r'-{3,}', text)
    for block in cpu_blocks:
        cpu_match = re.search(r'–ü–∏–∫ CPU.*?–ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞:\s*`([\d\.]+)%`', block)
        if cpu_match:
            cpu_usages.append(float(cpu_match.group(1)))
        # –ü–∞—Ä—Å–∏–º —Ç–∞–±–ª–∏—Ü—É –∑–∞–ø—Ä–æ—Å–æ–≤
        table_match = re.search(r'\|\s*ID\s*\|.*?\n((?:\|.*?\n)+)', block, re.DOTALL)
        if table_match:
            table = table_match.group(1)
            # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫–∏ —Ç–∞–±–ª–∏—Ü—ã
            for line in table.strip().split('\n'):
                if not line.strip().startswith('|'):
                    continue
                parts = [p.strip() for p in line.strip('|').split('|')]
                if len(parts) < 7:
                    continue
                try:
                    q_id, user, host, db, command, time_val, state, info = parts[:8]
                    time_val = int(time_val)
                    query = {
                        'ID': q_id,
                        'USER': user,
                        'HOST': host,
                        'DB': db,
                        'COMMAND': command,
                        'TIME': time_val,
                        'STATE': state,
                        'INFO': info
                    }
                    all_queries.append(query)
                    query_times.append(time_val)
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ INFO (–æ–±—Ä–µ–∑–∞–µ–º –¥–æ 100 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏)
                    group_key = info[:100]
                    query_groups[group_key].append(query)
                    if time_val > 30:
                        critical_queries.append(query)
                    elif time_val > 1:
                        slow_queries.append(query)
                except Exception:
                    continue
    # –ê–≥—Ä–µ–≥–∞—Ç—ã
    cpu_agg = {
        'max': max(cpu_usages) if cpu_usages else None,
        'min': min(cpu_usages) if cpu_usages else None,
        'avg': sum(cpu_usages)/len(cpu_usages) if cpu_usages else None,
        'count': len(cpu_usages)
    }
    query_time_agg = {
        'max': max(query_times) if query_times else None,
        'min': min(query_times) if query_times else None,
        'avg': sum(query_times)/len(query_times) if query_times else None,
        'count': len(query_times)
    }
    # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    grouped_queries = []
    for key, group in query_groups.items():
        grouped_queries.append({
            'INFO': key,
            'count': len(group),
            'avg_time': sum(q['TIME'] for q in group)/len(group),
            'max_time': max(q['TIME'] for q in group),
            'min_time': min(q['TIME'] for q in group)
        })
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É
    grouped_queries = sorted(grouped_queries, key=lambda x: x['count'], reverse=True)
    return {
        'cpu_agg': cpu_agg,
        'query_time_agg': query_time_agg,
        'grouped_queries': grouped_queries,
        'slow_queries': slow_queries,
        'critical_queries': critical_queries
    }

def generate_daily_summary_report(baseline_path, events_path, output_path):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –¥–Ω–µ–≤–Ω–æ–π –æ—Ç—á—ë—Ç —Å AI-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –∏ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Å–≤–æ–¥–∫–æ–π.
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è AI
    prompt = build_ai_prompt(baseline_path, events_path)
    if ENABLE_AI:
        try:
            ai_recommendations = send_to_ai_advisor(prompt)
        except Exception as e:
            ai_recommendations = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ AI: {e}"
    else:
        ai_recommendations = 'AI –æ—Ç–∫–ª—é—á—ë–Ω –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏.'
    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è
    agg = parse_and_aggregate_events(events_path)
    # –§–æ—Ä–º–∏—Ä—É–µ–º baseline-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ, –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ baseline)
    key_params = prompt.split('–í–æ—Ç —Å–≤–æ–¥–∫–∞ —Å–æ–±—ã—Ç–∏–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è:')[0].replace('–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä MySQL. –í–æ—Ç –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Å–µ—Ä–≤–µ—Ä–∞:', '').strip()
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É
    summary = []
    cpu_agg = agg.get('cpu_agg', {})
    if cpu_agg.get('count'):
        summary.append(f"**CPU:** —Å—Ä–µ–¥–Ω–µ–µ: {cpu_agg['avg']:.1f}%, –º–∞–∫—Å: {cpu_agg['max']}%, –º–∏–Ω: {cpu_agg['min']}% (–ø–∏–∫–æ–≤: {cpu_agg['count']})")
    query_time_agg = agg.get('query_time_agg', {})
    if query_time_agg.get('count'):
        summary.append(f"**–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤:** —Å—Ä–µ–¥–Ω–µ–µ: {query_time_agg['avg']:.1f} —Å–µ–∫, –º–∞–∫—Å: {query_time_agg['max']} —Å–µ–∫, –º–∏–Ω: {query_time_agg['min']} —Å–µ–∫ (–≤—Å–µ–≥–æ: {query_time_agg['count']})")
    # –ü–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã
    if agg.get('grouped_queries'):
        summary.append("**–ì—Ä—É–ø–ø—ã –ø–æ—Ö–æ–∂–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–ø–æ INFO):**")
        for g in agg['grouped_queries'][:5]:
            summary.append(f"- {g['INFO']} (–≤—Å–µ–≥–æ: {g['count']}, —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {g['avg_time']:.1f} —Å–µ–∫, –º–∞–∫—Å: {g['max_time']} —Å–µ–∫, –º–∏–Ω: {g['min_time']} —Å–µ–∫)")
    # –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ
    if agg.get('critical_queries'):
        summary.append("**–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (>30 —Å–µ–∫):**")
        for q in agg['critical_queries']:
            summary.append(f"- {q['INFO']} (–≤—Ä–µ–º—è: {q['TIME']} —Å–µ–∫)")
    if agg.get('slow_queries'):
        summary.append("**–ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (>10 —Å–µ–∫):**")
        for q in agg['slow_queries']:
            summary.append(f"- {q['INFO']} (–≤—Ä–µ–º—è: {q['TIME']} —Å–µ–∫)")
    # –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–ø–æ INFO)
    all_infos = set(q['INFO'] for q in agg.get('grouped_queries', []))
    if all_infos:
        summary.append("\n**–í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∑–∞ –¥–µ–Ω—å (–ø–æ INFO):**")
        for info in all_infos:
            summary.append(f"- {info}")
    summary_str = '\n'.join(summary)
    # –ò—Ç–æ–≥–æ–≤—ã–π markdown-–æ—Ç—á—ë—Ç
    report = f"""
# –°–≤–æ–¥–Ω—ã–π –æ—Ç—á—ë—Ç –∑–∞ {datetime.now().strftime('%Y-%m-%d')}

## –ö–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã MySQL
{key_params}

## –ò—Ç–æ–≥–æ–≤–∞—è —Å–≤–æ–¥–∫–∞ –∑–∞ –¥–µ–Ω—å
{summary_str}

## AI-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –Ω–µ–π—Ä–æ—Å–µ—Ç—å—é)
{ai_recommendations}
"""
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(report)
    return report 